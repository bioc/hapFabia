\name{iterateSegments}

\alias{iterateSegments}

\title{iterateSegments: Loop over all segments which calls \code{hapFabia}}

\description{

  \code{iterateSegments}: \R implementation of \code{iterateSegments}.

  Loops over all segments and calls \code{hapFabia}. Segments have been
  generated by \code{split_sparse_matrix}.


}
\usage{

iterateSegments(startRun=1,endRun,shift=5000,segmentSize=10000,
   annotationFile=NULL,fileName,prefixPath="",
   sparseMatrixPostfix="_mat",annotPostfix="_annot.txt",
   individualsPostfix="_individuals.txt",individuals=0,
   lowerBP=0,upperBP=0.05,p=10,iter=40,quant=0.01,eps=1e-5,
   alpha=0.03,cyc=50,non_negative=1,write_file=0,norm=0,
   lap=100.0,haploClusterLength=50,Lt = 0.1,Zt = 0.2,
   thresCount=1e-5,mintagSNVsFactor=3/4,pMAF=0.03,
   haplotypes=TRUE)

}
\arguments{
 \item{startRun}{first segment.}
 \item{endRun}{last segment.}
 \item{shift}{distance between starts of adjacent segments.}
 \item{segmentSize}{number of SNVs in a segment.}
 \item{annotationFile}{file name of the annotation file.}
 \item{fileName}{passed to hapFabia: file name of the genotype matrix in sparse format.}
 \item{prefixPath}{passed to hapFabia: path to the genotype file.}
 \item{sparseMatrixPostfix}{passed to hapFabia: postfix string for the sparse matrix.}
 \item{annotPostfix}{passed to hapFabia: postfix string for the annotation file.}
 \item{individualsPostfix}{passed to hapFabia: postfix string for the
   file containing the names of the individuals.}
 \item{individuals}{passed to hapFabia: vector of individuals which are included into the analysis; default = 0 (all individuals).}
 \item{lowerBP}{passed to hapFabia: lower bound on minor allele frequencies (MAF); however at least two occurrences are required to remove private SNPs.}
 \item{upperBP}{passed to hapFabia: upper bound  on minor allele frequencies (MAF) to extract rare variants.}
 \item{p}{passed to hapFabia: number of biclusters per iteration.}
 \item{iter}{passed to hapFabia: number of iterations.}
 \item{quant}{passed to hapFabia: percentage of loadings L to remove in each iteration.}
 \item{eps}{passed to hapFabia: lower bound for variational parameter lapla; default 1e-5.}
 \item{alpha}{passed to hapFabia: sparseness of the loadings; default = 0.03.}
 \item{cyc}{passed to hapFabia: number of cycles per iterations; default 50.}
 \item{non_negative}{non-negative factors and loadings if non_negative = 1; default = 1 (yes).}
 \item{write_file}{passed to hapFabia: results are written to files (L in sparse format), default = 0 (not written).}
 \item{norm}{passed to hapFabia: data normalization; default 1 (no normalization).}
 \item{lap}{passed to hapFabia: minimal value of the variational parameter; default 100.0.}
 \item{haploClusterLength}{passed to hapFabia: typical haplotype cluster length in kbp.}
 \item{Lt}{passed to hapFabia: percentage of largest Ls to consider for haplotype cluster extraction.}
 \item{Zt}{passed to hapFabia: percentage of largest Zs to consider for haplotype cluster extraction.}
 \item{thresCount}{passed to hapFabia: p-value of random histogram hit; default 1e-5.}
 \item{mintagSNVsFactor}{passed to hapFabia: percentage of haplotype cluster overlap; default 3/4.}
 \item{pMAF}{passed to hapFabia: averaged and corrected (for non-uniform distributions) minor allele frequency.}
\item{haplotypes}{haplotypes = TRUE then phased genotypes meaning two chromosomes per individual otherwise unphased genotypes.}


}
\details{


  Implementation in \R.
  Reads annotation of the samples if available,
  then calls \code{hapFabia}.
  Results are saved in EXCEL format and as \R
  binaries.

}
\seealso{
\code{\link{HaploCluster-class}},
\code{\link{HaploClusterList-class}},
\code{\link{analyzeHaploClusters}},
\code{\link{compareHaploClusterLists}},
\code{\link{extractHaploClusters}},
\code{\link{findDenseRegions}},
\code{\link{hapFabia}},
\code{\link{hapFabiaVersion}},
\code{\link{hapRes}},
\code{\link{chr1ASW1000G}},
\code{\link{haploClusterList2excel}},
\code{\link{identifyDuplicates}},
\code{\link{iterateSegments}},
\code{\link{makePipelineFile}},
\code{\link{matrixPlot}},
\code{\link{mergeHaploClusterLists}},
\code{\link{mergedHaploClusterList}},
\code{\link{plotHaplotypeCluster}},
\code{\link{res}},
\code{\link{setAnnotation}},
\code{\link{setStatistics}},
\code{\link{sim}},
\code{\link{simu}},
\code{\link{simulateHaploClustersFabia}},
\code{\link{simulateHaplotypeClusters}},
\code{\link{split_sparse_matrix}},
\code{\link{toolsFactorizationClass}},
\code{\link{vcftoFABIA}}
}
\author{Sepp Hochreiter}
\examples{

#Work in a temporary directory.

old_dir <- getwd()
setwd(tempdir())


# Load data and write to vcf file.
data(chr1ASW1000G)
write(chr1ASW1000G,file="chr1ASW1000G.vcf")

#Create the analysis pipeline.
makePipelineFile(fileName="chr1ASW1000G",shiftSize=500,segmentSize=1000)

source("pipeline.R")

# Following files are produced:
list.files(pattern="chr1")



# Next we load segment 5 and there the first and second haplotype cluster
posAll <- 5
start <- (posAll-1)*shiftSize
end <- start + segmentSize
pRange <- paste("_",format(start,scientific=FALSE),"_",format(end,scientific=FALSE),sep="")
load(file=paste(fileName,pRange,"_resAnno",".Rda",sep=""))
haploClusterList <- resHapFabia$mergedHaploClusterList
summary(haploClusterList)
haploCluster1 <- haploClusterList[[1]]
summary(haploCluster1)
haploCluster2 <- haploClusterList[[2]]
summary(haploCluster2)




#Plot the first haplotype cluster in segment 5
plot(haploCluster1,filename=paste(fileName,pRange,"_mat",sep=""))


#Plot the second haplotype cluster in segment 5
plot(haploCluster2,filename=paste(fileName,pRange,"_mat",sep=""))

setwd(old_dir)


\dontrun{

###here an example of the the automatically generated pipeline
### with: shiftSize=5000,segmentSize=10000,fileName="filename"


#####define segments, overlap, filename #######
shiftSize <- 5000
segmentSize <- 10000
fileName="filename" # without type

#####load libraries#######
library(hapFabia)
library(fabia)

#####convert from .vcf to _mat.txt: step 2. above#######
vcftoFABIA(fileName=fileName)

#####split/ generate segments: step 3. above#######
split_sparse_matrix(fileName=fileName,segmentSize=segmentSize,
shiftSize=shiftSize,annotation=TRUE)

#####compute how many segments we have#######
ina <- as.numeric(readLines(paste(fileName,"_mat.txt",sep=""),n=2))
snps <- ina[2]
over <- segmentSize%/%shiftSize
N1 <- snps%/%shiftSize
endRunA <- (N1-over+2)

#####analyze each segment#######
#####may be done by parallel runs#######
iterateSegments(startRun=1,endRun=endRunA,shift=shiftSize,
segmentSize=segmentSize,fileName=fileName,individuals=0,
upperBP=0.05,p=10,iter=40,alpha=0.03,cyc=50,haploClusterLength=50,
Lt = 0.1,Zt = 0.2,thresCount=1e-5,mintagSNVsFactor=3/4,
pMAF=0.035,haplotypes=TRUE)

#####identify duplicates#######
identifyDuplicates(fileName=fileName,startRun=1,endRun=endRunA,
shift=shiftSize,segmentSize=segmentSize)

#####analyze results; parallel#######
anaRes <- analyzeHaploClusters(fileName=fileName,startRun=1,endRun=endRunA,
shift=shiftSize,segmentSize=segmentSize)
print("Number haplotype clusters:")
print(anaRes$nohaploClusters)
print("Average haplotye cluster length:")
print(anaRes$avhaploClusterLengthS)
print("Average number of individuals belonging to haplotye clusters:")
print(anaRes$avnoIndividS)
print("Average number of tagSNVs of haplotye clusters:")
print(anaRes$avnoTagSNVsS)
print("Average MAF of tagSNVs of haplotye clusters:")
print(anaRes$avnoFreqS)
print("Average MAF within the group of tagSNVs of haplotye clusters:")
print(anaRes$avnoGroupFreqS)
print("Average number of changes between major and minor allele frequency:")
print(anaRes$avnotagSNVChangeS)
print("Averge number of tagSNVs per individual of a haplotype cluster:")
print(anaRes$avnotagSNVsPerIndividualS)
print("Average number of individuals that have the minor allele of tagSNVs:")
print(anaRes$avnoindividualPerTagSNVS)

#####load result for segment 50#######
posAll <- 50 # (50-1)*5000 = 245000: segment 245000 to 255000
start <- (posAll-1)*shiftSize
end <- start + segmentSize
pRange <- paste("_",format(start,scientific=FALSE),"_",
format(end,scientific=FALSE),sep="")
load(file=paste(fileName,pRange,"_resAnno",".Rda",sep=""))
haploClusterList <- resHapFabia$mergedHaploClusterList # $

summary(haploClusterList)
#####plot haplotype clusters in segment 50#######
plot(haploClusterList,filename=paste(fileName,pRange,"_mat",sep=""))
   ##attention: filename without type ".txt"

#####plot the first haplotype cluster in segment 50#######

haploCluster <- haploClusterList[[1]]
plot(haploCluster,filename=paste(fileName,pRange,"_mat",sep=""))
   ##attention: filename without type ".txt"

}


}
\references{

S. Hochreiter et al.,
\sQuote{FABIA: Factor Analysis for Bicluster Acquisition},
Bioinformatics 26(12):1520-1527, 2010.

}
\keyword{models,multivariate,cluster}
\concept{genetics,haplotype,identity by descent,bicluster,next generation sequencing,genotype,single nucleotide polymorphism,single nucleotide variation,rare variants,rare SNPs, rare SNVs,rare haplotype clusters,short haplotype clusters}
